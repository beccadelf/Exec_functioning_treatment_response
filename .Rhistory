combined_results[[task]] <- results_corrected_homogeneity[[task]]
}
for (task in names(results_corrected_both)) {
combined_results[[task]] <- results_corrected_both[[task]]
}
# Combine results to a table
table_summary <- summarize_ancova_results(results_list = combined_results)
# Apply correction for multiple comparisons
table_summary$p_values_adjusted <- p.adjust(table_summary$p_value, method = "BH")
table_summary$F_value <- round(table_summary$F_value,2)
table_summary$error_reduction_due_to_predictor <- round(table_summary$error_reduction_due_to_predictor,2)
table_summary$p_value <- round(table_summary$p_value,3)
table_summary
# Change taks names and column names
rename_map <- c(
'NumberLetter_BIS_Repeat' = "Number-Letter: Repeat",
'NumberLetter_BIS_Switch' = "Number-Letter: Switch",
'NumberLetter_BIS_Diff_Score' = "Number-Letter: Switch - Repeat",
'Stroop_BIS_Congruent' = "Stroop: Congruent",
'Stroop_BIS_Incongruent' = "Stroop: Incongruent",
'Stroop_BIS_Diff_Score' = "Stroop: Incongruent - Congruent",
'TwoBack_BIS_Foil' = "2-Back: Foil",
'TwoBack_BIS_Target' = "2-Back: Target",
'TwoBack_BIS_Total' = "2-Back: Total",
'SSRT' = "Stop-Signal RT")
table_summary <- table_summary %>%mutate(Name = dplyr::recode(Name, !!!rename_map))
#table_summary$Name <- recode(table_summary$Name, !!!rename_map)
colnames(table_summary) <- dplyr::recode(colnames(table_summary),"Name" = "Performance Measure",
"error_reduction_due_to_predictor" = "Type III SSq explained by Group",
"F_value" = "F-Value",
"p_value" = "p-Value",
"partial_omega_squared" = "Partial Omega Squared",
"correction_for" = "Corrections",
"p_values_adjusted" = "adj. p-Value")
ft_t_test <- flextable::flextable(table_summary)
format_flextable_portrait <- flextable_settings(word_orientation = "portrait")
# Set table properties
ft_t_test <- set_table_properties(ft_t_test, width = 1, layout = "autofit")
# Round
ft_t_test  <- colformat_double(ft_t_test , digits = 2)
# Header in bold
ft_t_test <- bold(ft_t_test, bold = TRUE, part = "header")
# Alignments
ft_t_test <- align(ft_t_test, j = 1, align = "left", part = "all") # first column
ft_t_test <- align(ft_t_test, j = 2:ncol(table_summary), align = "center", part = "all") # rest
save_as_docx(
ft_t_test,
path = file.path(results_path,"ANCOVA.docx"),
pr_section = format_flextable_portrait)
ft_t_test
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
family = "Arial"
theme_set(theme_bw() + theme(text = element_text(family = family),
axis.title = element_text(size = 24),
axis.title.y = element_text(size = 24, margin = margin(r = 30)),
axis.text = element_text(size = 18),
#legend.text = element_text(size = 18),
strip.text = element_text(size = 20)
#plot.title = element_text(size = size_text, family = family)
#panel.spacing.x = unit(3, "lines")
))
# or theme_classic()
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders <- results_folders[grepl("final", results_folders)& !grepl("permuted", results_folders)]
# For each folder, save the performance_across_iters-file in dictionary
results_dict = list()
for (folder_name in results_folders) {
# Read the file and store it as a dictionary
results_dict[[folder_name]] <- read.delim(file.path(path_results_folder, folder_name,
"performance_across_iters.txt"))
}
# Combine results
combined_results <- bind_rows(results_dict, .id = "FolderName")
combined_results$X <- NULL
# Separate into regressor and classifier dataframes
classification_results <- combined_results[grepl("classifier", combined_results$FolderName), ]
regression_results <- combined_results[grepl("regressor", combined_results$FolderName), ]
create_classification_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_classifier_yes_smote_oversampling", results_df$FolderName),
"Random Forest\n(OS: SMOTE)",
ifelse(grepl("random_forest_classifier_yes_simple", results_df$FolderName),
"Random Forest\n(OS: Simple)",
ifelse(grepl("random_forest_classifier_no_oversampling", results_df$FolderName),
"Random Forest\n(OS: no)",
ifelse(grepl("svm_classifier_yes_smote", results_df$FolderName),
"Rbf-SVM\n(OS: SMOTE)",
ifelse(grepl("svm_classifier_yes_simple", results_df$FolderName),
"Rbf-SVM\n(OS: Simple)",
ifelse(grepl("svm_classifier_no_oversampling", results_df$FolderName),
"Rbf-SVM\n(OS: no)", NA))))))
# Ensure correct ordering
results_df$Model <- factor(results_df$Model, levels = c("Random Forest\n(OS: SMOTE)","Random Forest\n(OS: Simple)","Random Forest\n(OS: no)","Rbf-SVM\n(OS: SMOTE)",
"Rbf-SVM\n(OS: Simple)","Rbf-SVM\n(OS: no)"))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=balanced_accuracy, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Balanced Accuracy",  x = "", fill = "Feature Set") +
scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)) +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
geom_rect(data = subset(results_df, Model == "Random Forest\n(OS: SMOTE)"),
fill = NA, colour = "black", size = 1.5, xmin = -Inf,xmax = Inf,
ymin = -Inf,ymax = Inf) +
facet_wrap(~Model, nrow = 1)
return(plot)
}
classification_plot <- create_classification_plot(results_df = classification_results)
create_regression_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_regressor", results_df$FolderName), "Random Forest",
ifelse(grepl("ridge_regressor", results_df$FolderName), "Ridge", NA))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=MSE, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Mean Squared Error",  x = "", fill = "Feature Set") +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
facet_wrap(~Model, nrow = 1)
return(plot)
}
regression_plot <- create_regression_plot(results_df = regression_results)
plots_path = file.path(basic_path, "Projekte_Meinke/Old_projects/Labrotation_Rebecca/2_Machine_learning/Plots/")
ggsave(file.path(plots_path,"Classification_results_plot.png"), plot = classification_plot, width = 16, height = 9, dpi = 300)
ggsave(file.path(plots_path, "Regression_results_plot.png"), plot = regression_plot, width = 16, height = 9, dpi = 300)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
family = "Arial"
theme_set(theme_bw() + theme(text = element_text(family = family),
axis.title = element_text(size = 24),
axis.title.y = element_text(size = 24, margin = margin(r = 30)),
axis.text = element_text(size = 18),
#legend.text = element_text(size = 18),
strip.text = element_text(size = 20)
#plot.title = element_text(size = size_text, family = family)
#panel.spacing.x = unit(3, "lines")
))
# or theme_classic()
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders <- results_folders[grepl("final", results_folders)& !grepl("permuted", results_folders)]
# For each folder, save the performance_across_iters-file in dictionary
results_dict = list()
for (folder_name in results_folders) {
# Read the file and store it as a dictionary
results_dict[[folder_name]] <- read.delim(file.path(path_results_folder, folder_name,
"performance_across_iters.txt"))
}
# Combine results
combined_results <- bind_rows(results_dict, .id = "FolderName")
combined_results$X <- NULL
# Separate into regressor and classifier dataframes
classification_results <- combined_results[grepl("classifier", combined_results$FolderName), ]
regression_results <- combined_results[grepl("regressor", combined_results$FolderName), ]
create_classification_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_classifier_yes_smote_oversampling", results_df$FolderName),
"Random Forest\n(OS: SMOTE)",
ifelse(grepl("random_forest_classifier_yes_simple", results_df$FolderName),
"Random Forest\n(OS: Simple)",
ifelse(grepl("random_forest_classifier_no_oversampling", results_df$FolderName),
"Random Forest\n(OS: no)",
ifelse(grepl("svm_classifier_yes_smote", results_df$FolderName),
"Rbf-SVM\n(OS: SMOTE)",
ifelse(grepl("svm_classifier_yes_simple", results_df$FolderName),
"Rbf-SVM\n(OS: Simple)",
ifelse(grepl("svm_classifier_no_oversampling", results_df$FolderName),
"Rbf-SVM\n(OS: no)", NA))))))
# Ensure correct ordering
results_df$Model <- factor(results_df$Model, levels = c("Random Forest\n(OS: SMOTE)","Random Forest\n(OS: Simple)","Random Forest\n(OS: no)","Rbf-SVM\n(OS: SMOTE)",
"Rbf-SVM\n(OS: Simple)","Rbf-SVM\n(OS: no)"))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=balanced_accuracy, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Balanced Accuracy",  x = "", fill = "Feature Set") +
scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)) +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
geom_rect(data = subset(results_df, Model == "Random Forest\n(OS: SMOTE)"),
fill = NA, colour = "black", size = 1.5, xmin = -Inf,xmax = Inf,
ymin = -Inf,ymax = Inf) +
facet_wrap(~Model, nrow = 1)
return(plot)
}
classification_plot <- create_classification_plot(results_df = classification_results)
create_regression_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_regressor", results_df$FolderName), "Random Forest",
ifelse(grepl("ridge_regressor", results_df$FolderName), "Ridge", NA))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=MSE, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Mean Squared Error",  x = "", fill = "Feature Set") +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
facet_wrap(~Model, nrow = 1)
return(plot)
}
regression_plot <- create_regression_plot(results_df = regression_results)
plots_path = file.path(basic_path, "Projekte_Meinke/Old_projects/Labrotation_Rebecca/2_Machine_learning/Plots/")
ggsave(file.path(plots_path,"Classification_results_plot.png"), plot = classification_plot, width = 16, height = 9, dpi = 300)
ggsave(file.path(plots_path, "Regression_results_plot.png"), plot = regression_plot, width = 16, height = 9, dpi = 300)
pander(collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders_classification <- results_folders[grepl("final", results_folders)& grepl("classifier", results_folders) & !grepl("permuted", results_folders)]
results_folders_regressors <- results_folders[grepl("final", results_folders)& grepl("regressor", results_folders) & !grepl("permuted", results_folders)]
calc_p_value <- function(true_score, permuted_scores, logic = "large_better"){
if (logic == "large_better"){
random_better_than_true <- sum(permuted_scores > true_score)
} else if (logic == "small_better"){
random_better_than_true <- sum(permuted_scores < true_score)
}
#Take the following formula from scikit: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score
p_value <- (random_better_than_true + 1)/(length(permuted_scores) +1 )
return(p_value)
}
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
return(df)
}
pander(collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy"))
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("features"," ",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features"," ",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features","",df$models)
df
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
rownames(df) <- NULL
return(df)
}
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders_classification <- results_folders[grepl("final", results_folders)& grepl("classifier", results_folders) & !grepl("permuted", results_folders)]
results_folders_regressors <- results_folders[grepl("final", results_folders)& grepl("regressor", results_folders) & !grepl("permuted", results_folders)]
calc_p_value <- function(true_score, permuted_scores, logic = "large_better"){
if (logic == "large_better"){
random_better_than_true <- sum(permuted_scores > true_score)
} else if (logic == "small_better"){
random_better_than_true <- sum(permuted_scores < true_score)
}
#Take the following formula from scikit: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score
p_value <- (random_better_than_true + 1)/(length(permuted_scores) +1 )
return(p_value)
}
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
rownames(df) <- NULL
return(df)
}
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features","",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_regressors, evaluation_metric = "MSE", logic = "small_better")
df$models <- gsub("random_forest_regressopr","rf",df$models)
df$models <- gsub("ridge_regressor","ridge",df$models)
df$models <- gsub("_features","",df$models)
df
library(rmarkdown)
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Useful_functions.R")
# Define the base path
base_path <- "Y:/PsyThera/Projekte_Meinke/Old_projects/Labrotation_Rebecca/0_Datapreparation"
parent_path <- dirname(base_path)
# General further processing
inputdata_variants_paths <- c(
file.path(base_path, "not_trimmed_not_removed/BIS/outliers-not-removed"),
file.path(base_path, "not_trimmed_not_removed/BIS/outliers-removed"),
file.path(base_path, "RT_trimmed_RT_wrong_removed/BIS/outliers-not-removed"),
file.path(base_path, "RT_trimmed_RT_wrong_removed/BIS/outliers-removed")
)
# Generate htmlfilename for the group comparison and machine learning script
generate_htmlfilename_analyses <- function(input_data_path,prefix) {
# This function creates the html-filename based on the preprocessing of the input data
# Get the name of the input data folder as it tells us whether outliers were removed
last_folder <- basename(input_data_path)
# Get the second-to-last folder as it tells how the mean RT was calculated
parent_dir <- dirname(input_data_path)
grandparent_dir <- dirname(parent_dir)
third_last_folder <- basename(grandparent_dir)
paste0(prefix,"_", third_last_folder, "_", last_folder,".html")
}
# Group comparison script (Response vs. Nonresponse)
for (input_data_path in inputdata_variants_paths) {
params_list <- list(input_data_path = input_data_path)
output_filename <- generate_htmlfilename_analyses(input_data_path, prefix = "Response_vs_Nonresponse")
output_path <- file.path(create_results_path(inputdata_path = input_data_path,
output_mainpath = file.path(parent_path,"1_Group_comparison/R_vs_NR")),
output_filename)
rmarkdown::render(
input = "Group Comparison_Executive Functions\\Group Comparison_Response Nonresponse.Rmd",
output_file = output_path,
params = params_list,
envir = new.env()
)
cat("Generated file:", output_filename, "\n")
}
