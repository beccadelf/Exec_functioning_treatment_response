ifelse(grepl("svm_classifier_yes_simple", results_df$FolderName),
"Rbf-SVM\n(OS: Simple)",
ifelse(grepl("svm_classifier_no_oversampling", results_df$FolderName),
"Rbf-SVM\n(OS: no)", NA))))))
# Ensure correct ordering
results_df$Model <- factor(results_df$Model, levels = c("Random Forest\n(OS: SMOTE)","Random Forest\n(OS: Simple)","Random Forest\n(OS: no)","Rbf-SVM\n(OS: SMOTE)",
"Rbf-SVM\n(OS: Simple)","Rbf-SVM\n(OS: no)"))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=balanced_accuracy, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Balanced Accuracy",  x = "", fill = "Feature Set") +
scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)) +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
geom_rect(data = subset(results_df, Model == "Random Forest\n(OS: SMOTE)"),
fill = NA, colour = "black", size = 1.5, xmin = -Inf,xmax = Inf,
ymin = -Inf,ymax = Inf) +
facet_wrap(~Model, nrow = 1)
return(plot)
}
classification_plot <- create_classification_plot(results_df = classification_results)
create_regression_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_regressor", results_df$FolderName), "Random Forest",
ifelse(grepl("ridge_regressor", results_df$FolderName), "Ridge", NA))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=MSE, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Mean Squared Error",  x = "", fill = "Feature Set") +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
facet_wrap(~Model, nrow = 1)
return(plot)
}
regression_plot <- create_regression_plot(results_df = regression_results)
plots_path = file.path(basic_path, "Projekte_Meinke/Old_projects/Labrotation_Rebecca/2_Machine_learning/Plots/")
ggsave(file.path(plots_path,"Classification_results_plot.png"), plot = classification_plot, width = 16, height = 9, dpi = 300)
ggsave(file.path(plots_path, "Regression_results_plot.png"), plot = regression_plot, width = 16, height = 9, dpi = 300)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
family = "Arial"
theme_set(theme_bw() + theme(text = element_text(family = family),
axis.title = element_text(size = 24),
axis.title.y = element_text(size = 24, margin = margin(r = 30)),
axis.text = element_text(size = 18),
#legend.text = element_text(size = 18),
strip.text = element_text(size = 20)
#plot.title = element_text(size = size_text, family = family)
#panel.spacing.x = unit(3, "lines")
))
# or theme_classic()
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders <- results_folders[grepl("final", results_folders)& !grepl("permuted", results_folders)]
# For each folder, save the performance_across_iters-file in dictionary
results_dict = list()
for (folder_name in results_folders) {
# Read the file and store it as a dictionary
results_dict[[folder_name]] <- read.delim(file.path(path_results_folder, folder_name,
"performance_across_iters.txt"))
}
# Combine results
combined_results <- bind_rows(results_dict, .id = "FolderName")
combined_results$X <- NULL
# Separate into regressor and classifier dataframes
classification_results <- combined_results[grepl("classifier", combined_results$FolderName), ]
regression_results <- combined_results[grepl("regressor", combined_results$FolderName), ]
create_classification_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_classifier_yes_smote_oversampling", results_df$FolderName),
"Random Forest\n(OS: SMOTE)",
ifelse(grepl("random_forest_classifier_yes_simple", results_df$FolderName),
"Random Forest\n(OS: Simple)",
ifelse(grepl("random_forest_classifier_no_oversampling", results_df$FolderName),
"Random Forest\n(OS: no)",
ifelse(grepl("svm_classifier_yes_smote", results_df$FolderName),
"Rbf-SVM\n(OS: SMOTE)",
ifelse(grepl("svm_classifier_yes_simple", results_df$FolderName),
"Rbf-SVM\n(OS: Simple)",
ifelse(grepl("svm_classifier_no_oversampling", results_df$FolderName),
"Rbf-SVM\n(OS: no)", NA))))))
# Ensure correct ordering
results_df$Model <- factor(results_df$Model, levels = c("Random Forest\n(OS: SMOTE)","Random Forest\n(OS: Simple)","Random Forest\n(OS: no)","Rbf-SVM\n(OS: SMOTE)",
"Rbf-SVM\n(OS: Simple)","Rbf-SVM\n(OS: no)"))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=balanced_accuracy, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Balanced Accuracy",  x = "", fill = "Feature Set") +
scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)) +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
geom_rect(data = subset(results_df, Model == "Random Forest\n(OS: SMOTE)"),
fill = NA, colour = "black", size = 1.5, xmin = -Inf,xmax = Inf,
ymin = -Inf,ymax = Inf) +
facet_wrap(~Model, nrow = 1)
return(plot)
}
classification_plot <- create_classification_plot(results_df = classification_results)
create_regression_plot <- function(results_df){
# Create new model variable
results_df$Model <- ifelse(grepl("random_forest_regressor", results_df$FolderName), "Random Forest",
ifelse(grepl("ridge_regressor", results_df$FolderName), "Ridge", NA))
# Define feature sets and specify colors
results_df$FeatureSet <- ifelse(grepl("clinical_features", results_df$FolderName),
"Clinical Features", "Clinical + Executive\nFunctioning Features")
colors <- c("Clinical Features" = "white", "Clinical + Executive\nFunctioning Features" = "#D3D3D3")
# Set factor levels for FeatureSet to control order in the plot
results_df$FeatureSet <- factor(results_df$FeatureSet, levels = c("Clinical Features", "Clinical + Executive\nFunctioning Features"))
# Create ggplot
plot <- ggplot(results_df, aes(x=FeatureSet, y=MSE, fill=FeatureSet)) +
geom_violin(scale = "width") +
geom_jitter(size = 1, width = 0.2, height = 0) +
stat_summary(
geom = "point",
shape = 24,
fun = "mean",
col = "black",
fill = "red",
size = 3
) +
labs(y = "Mean Squared Error",  x = "", fill = "Feature Set") +
scale_fill_manual(values = colors) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
facet_wrap(~Model, nrow = 1)
return(plot)
}
regression_plot <- create_regression_plot(results_df = regression_results)
plots_path = file.path(basic_path, "Projekte_Meinke/Old_projects/Labrotation_Rebecca/2_Machine_learning/Plots/")
ggsave(file.path(plots_path,"Classification_results_plot.png"), plot = classification_plot, width = 16, height = 9, dpi = 300)
ggsave(file.path(plots_path, "Regression_results_plot.png"), plot = regression_plot, width = 16, height = 9, dpi = 300)
pander(collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders_classification <- results_folders[grepl("final", results_folders)& grepl("classifier", results_folders) & !grepl("permuted", results_folders)]
results_folders_regressors <- results_folders[grepl("final", results_folders)& grepl("regressor", results_folders) & !grepl("permuted", results_folders)]
calc_p_value <- function(true_score, permuted_scores, logic = "large_better"){
if (logic == "large_better"){
random_better_than_true <- sum(permuted_scores > true_score)
} else if (logic == "small_better"){
random_better_than_true <- sum(permuted_scores < true_score)
}
#Take the following formula from scikit: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score
p_value <- (random_better_than_true + 1)/(length(permuted_scores) +1 )
return(p_value)
}
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
return(df)
}
pander(collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy"))
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("features"," ",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features"," ",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features","",df$models)
df
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
rownames(df) <- NULL
return(df)
}
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
windowsFonts(Arial=windowsFont("Arial"))
library(readr)
library(dplyr)
library(ggplot2)
# Get all folders in the results folder (each containing the results of another model)
basic_path = "Y:\\Psythera"
path_results_folder = file.path(basic_path, "Projekte_Meinke\\Old_projects\\Labrotation_Rebecca\\2_Machine_learning\\Results\\RT_trimmed_RT_wrong_removed_outliers-removed")
results_folders <- list.files(path = path_results_folder, full.names = FALSE)
# Filter for new results
results_folders_classification <- results_folders[grepl("final", results_folders)& grepl("classifier", results_folders) & !grepl("permuted", results_folders)]
results_folders_regressors <- results_folders[grepl("final", results_folders)& grepl("regressor", results_folders) & !grepl("permuted", results_folders)]
calc_p_value <- function(true_score, permuted_scores, logic = "large_better"){
if (logic == "large_better"){
random_better_than_true <- sum(permuted_scores > true_score)
} else if (logic == "small_better"){
random_better_than_true <- sum(permuted_scores < true_score)
}
#Take the following formula from scikit: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score
p_value <- (random_better_than_true + 1)/(length(permuted_scores) +1 )
return(p_value)
}
collect_p_values <- function(folders_unpermuted, evaluation_metric, logic = "large_better"){
p_values = list()
evaluation_metric_means = list()
for (folder_name in folders_unpermuted) {
# Initialize p_value to NA by default
p_values[[folder_name]] <- NA
evaluation_metric_means[[folder_name]] <- NA
# Get evaluation metrics of unpermuted model
evaluation_metrics <- read.delim(file.path(path_results_folder, folder_name, "performance_across_iters.txt"))[[evaluation_metric]]
if (length(evaluation_metrics) != 100){
print(paste("CAVE: There is an error in",folder_name))
}
# Get equivalent permuted model folder name
folder_name_permuted <- paste0(folder_name, "_permuted")
# Try to load the evaluation metric per iteration
tryCatch({
evaluation_metrics_permuted <- read.delim(file.path(path_results_folder, folder_name_permuted, "performance_across_iters.txt"))[[evaluation_metric]]
p_values_one_folder = c()
for (ev_metric_one_iter in evaluation_metrics){
p_value_one_iter = calc_p_value(ev_metric_one_iter, evaluation_metrics_permuted, logic = logic)
p_values_one_folder = c(p_values_one_folder, p_value_one_iter)
}
# Sum all p-values up
p_values[[folder_name]] = mean(p_values_one_folder)
}, error = function(e) {
# Handle error and print message if permuted model is not available
message("No permuted model available for folder: ", folder_name, " | Error: ", e$message)
# Set p_value to NA (this is already initialized)
p_values[[folder_name]] <- NA
})
# Collect true mean of evaluation metric
evaluation_metric_means[[folder_name]] <- paste0(round(mean(evaluation_metrics),2), "(",round(sd(evaluation_metrics),2),")")
}
df = data.frame("models" = names(p_values), "p_values" = unlist(p_values), evaluation_metrics_means = unlist(evaluation_metric_means))
rownames(df) <- NULL
return(df)
}
df <- collect_p_values(folders_unpermuted = results_folders_classification, evaluation_metric = "balanced_accuracy")
# Improve readibility of table
df$models <- gsub("random_forest_classifier","rf",df$models)
df$models <- gsub("svm_classifier","svm",df$models)
df$models <- gsub("_features","",df$models)
df
df <- collect_p_values(folders_unpermuted = results_folders_regressors, evaluation_metric = "MSE", logic = "small_better")
df$models <- gsub("random_forest_regressopr","rf",df$models)
df$models <- gsub("ridge_regressor","ridge",df$models)
df$models <- gsub("_features","",df$models)
df
library(rmarkdown)
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Useful_functions.R")
# Define the base path
base_path <- "Y:/PsyThera/Projekte_Meinke/Old_projects/Labrotation_Rebecca/0_Datapreparation"
parent_path <- dirname(base_path)
# General further processing
inputdata_variants_paths <- c(
file.path(base_path, "not_trimmed_not_removed/BIS/outliers-not-removed"),
file.path(base_path, "not_trimmed_not_removed/BIS/outliers-removed"),
file.path(base_path, "RT_trimmed_RT_wrong_removed/BIS/outliers-not-removed"),
file.path(base_path, "RT_trimmed_RT_wrong_removed/BIS/outliers-removed")
)
# Generate htmlfilename for the group comparison and machine learning script
generate_htmlfilename_analyses <- function(input_data_path,prefix) {
# This function creates the html-filename based on the preprocessing of the input data
# Get the name of the input data folder as it tells us whether outliers were removed
last_folder <- basename(input_data_path)
# Get the second-to-last folder as it tells how the mean RT was calculated
parent_dir <- dirname(input_data_path)
grandparent_dir <- dirname(parent_dir)
third_last_folder <- basename(grandparent_dir)
paste0(prefix,"_", third_last_folder, "_", last_folder,".html")
}
# Group comparison script (Response vs. Nonresponse)
for (input_data_path in inputdata_variants_paths) {
params_list <- list(input_data_path = input_data_path)
output_filename <- generate_htmlfilename_analyses(input_data_path, prefix = "Response_vs_Nonresponse")
output_path <- file.path(create_results_path(inputdata_path = input_data_path,
output_mainpath = file.path(parent_path,"1_Group_comparison/R_vs_NR")),
output_filename)
rmarkdown::render(
input = "Group Comparison_Executive Functions\\Group Comparison_Response Nonresponse.Rmd",
output_file = output_path,
params = params_list,
envir = new.env()
)
cat("Generated file:", output_filename, "\n")
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(gtsummary)
library(tidyr)
library(flextable)
# User-defined functions
source("../Useful_functions.R")
format_flextable_portrait <- flextable_settings(word_orientation = "portrait")
format_flextable_landscape <- flextable_settings(word_orientation = "landscape")
basic_path <- file.path(params$input_data_path, params$response_criterion)
data_HC <- read.csv(file.path(basic_path,"Data_HC.csv"))
basic_path <- file.path(params$input_data_path, params$response_criterion)
data_HC <- read.csv(file.path(basic_path,"Data_HC.csv"))
data_Pat_pre <- read.csv(file.path(basic_path,"Data_Patients_Pre.csv"))
output_mainpath_HC_Pat <- file.path(params$output_base_path, "HC_vs_Pat", params$response_criterion)
output_mainpath_Response <- file.path(params$output_base_path, "R_vs_NR", params$response_criterion)
results_path_HC_Pat <- create_results_path(inputdata_path = basic_path,
output_mainpath = output_mainpath_HC_Pat)
results_path_Response <- create_results_path(inputdata_path = basic_path,
output_mainpath = output_mainpath_Response)
data_Pat_pre_clean <- data_Pat_pre[!is.na(data_Pat_pre$Response),]
# Dummy-code variable education
recode_results_2 <- dummy_code_education(data_Pat_pre_clean)
data_Pat_pre_clean <- recode_results_2$data_recoded
categorical_vars <- c("Geschlecht", "Abschluss_Gymnasium")
dimensional_vars <- c("Alter", "T1_BAT_FAS_score", "T1_BAT_BDI_II_score", "T1_BAT_STAI_T_score", "T1_BAT_BIS_11_score", "T1_BAT_Kirby_k_score", "T1_BAT_CFC_14_score", "T1_BAT_SRHI_score")
t_test_descr_HC_Pat <- t_test_mult_cols(df_basis = Patients_vs_HC, cols = dimensional_vars, grouping_variable = "Gruppe")
# Merge patients pre and controls
Patients_vs_HC <- rbind(data_Pat_pre, data_HC)
# Dummy-code variable education
recode_results_1 <- dummy_code_education(Patients_vs_HC)
Patients_vs_HC <- recode_results_1$data_recoded
t_test_descr_HC_Pat <- t_test_mult_cols(df_basis = Patients_vs_HC, cols = dimensional_vars, grouping_variable = "Gruppe")
t_test_descr_Response <- t_test_mult_cols(df_basis = data_Pat_pre_clean, cols = dimensional_vars, grouping_variable = "Response")
chi_sq_test_descr_HC_Pat <- chi_sq_test_mult_cols(df_basis = Patients_vs_HC, cols = categorical_vars, grouping_variable = "Gruppe")
chi_sq_test_descr_Response <- chi_sq_test_mult_cols(df_basis = data_Pat_pre_clean, cols = categorical_vars, grouping_variable = "Response")
t_test_descr_pub_Response <- prepare_ttest_table(ttest_table = t_test_descr_Response, var_type = "dimensional", group_0 = "Non-Responder", group_1 = "Responder")
chi_sq_test_descr_pub_Response <- prepare_ttest_table(ttest_table = chi_sq_test_descr_Response, var_type = "categorical", group_0 = "Non-Responder", group_1 = "Responder")
# Merge the dataframes
descr_comps_Response <- rbind(t_test_descr_pub_Response, chi_sq_test_descr_pub_Response)
# Reorder and rename the variables
descr_comps_Response <- reorder_and_rename_rows(df = descr_comps_Response, col_name = "Variable", label_map = variable_labels)
# Print missings
for (i in seq_len(nrow(descr_comps_Response))) {
miss_NR <- descr_comps_Response$`Missings_Non-Responder`[i]
miss_R <- descr_comps_Response$Missings_Responder[i]
if (miss_NR > 0 || miss_R > 0) {
var_name <- descr_comps_Response$Variable[i]
cat(sprintf("Missing values **%s**: Non-Responder = %s, Responder = %s \n",
var_name, miss_NR, miss_R))
}
}
# Remove columns Missings_HC, Missings_Patients
descr_comps_Response <- descr_comps_Response[, !(names(descr_comps_Response) %in% c("Missings_Non-Responder", "Missings_Responder"))]
# Add number of subjects per group
n_NR <- sum(data_Pat_pre_clean$Response == 0)
n_R <- sum(data_Pat_pre_clean$Response == 1)
n_subjects_row <- data.frame(Variable = "N", `Non-Responder` = n_NR, Responder = n_R, Statistic = "-", stringsAsFactors = FALSE, check.names = FALSE)
descr_comps_Response <- rbind(n_subjects_row, descr_comps_Response)
View(descr_comps_Response)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(pander)
library(effsize)
library(pwr)
library(gtsummary)
library(gt)
library(car)
library(flextable)
library(officer)
# User-defined functions
source("../Useful_functions.R")
format_flextable_portrait <- flextable_settings(word_orientation = "portrait")
format_flextable_landscape <- flextable_settings(word_orientation = "landscape")
basic_path <- file.path(params$input_data_path, params$response_criterion)
data_Pat_pre <- read.csv(file.path(basic_path,"Data_Patients_Pre.csv"))
output_mainpath <- file.path(params$output_base_path, params$response_criterion)
results_path <- create_results_path(inputdata_path = basic_path,
output_mainpath = output_mainpath
)
data_Pat_pre_clean <- data_Pat_pre[!is.na(data_Pat_pre$Response),]
# Ensure "Response" is treated as a factor
data_Pat_pre_clean$Response <- as.factor(data_Pat_pre_clean$Response)
# Define the columns for which to perform the test (i.e., task performance measures)
imp_columns <- c("NumberLetter_BIS_Repeat", "NumberLetter_BIS_Switch",
"NumberLetter_BIS_Diff_Score", "Stroop_BIS_Congruent",
"Stroop_BIS_Incongruent", "Stroop_BIS_Diff_Score",
"TwoBack_BIS_Foil", "TwoBack_BIS_Target", "TwoBack_BIS_Total",
"SSRT")
t_test_table <- t_test_mult_cols(df_basis = data_Pat_pre_clean, cols = imp_columns, grouping_variable = "Response")
#pander(t_test_table, style = "rmarkdown", split.table = Inf, fontsize = "tiny")
t_test_table
View(t_test_table)
