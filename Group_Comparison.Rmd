---
title: "Group Comparison"
authors: "Charlotte Meinke, Rebecca Delfendahl, Till Julius Adam"
date: "2024-09-04"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---
<style type="text/css">
.main-container { /* Adjust main blocks */
  max-width: 100% !important;
  margin: auto;
}

body {
  font-family: "Georgia", serif !important; /* Set the font for the entire document */
}

.tocify { /* Adjust table of contents */
  max-width: 100% !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(pander)
library(effsize)
library(pwr)
library(gtsummary)
library(stats)
```

## <u>Importing Data</u>

```{r}
basic_path = "Z:/Projekte_Meinke/Old_projects/Labrotation_Rebecca/Daten_Gruppenvergleich/With_wrong_responses"
data_HC <- read.csv(file.path(basic_path,"Data_HC.csv"))
data_Pat_pre <- read.csv(file.path(basic_path,"Data_Patients_Pre.csv"))
data_Pat_post <- read.csv(file.path(basic_path,"Data_Patients_Post.csv"))
```

## <u>Comparison HC vs. Patients</u>

### Prepare dataset
```{r}
## 1. Merge patients pre and controls
Patients_vs_HC <- rbind(data_Pat_pre, data_HC)

## 2. Reduce dataset to executive functions of interest (BIS columns and SSRT)
BIS_columns <- colnames(Patients_vs_HC)[grep("BIS", colnames(Patients_vs_HC))]
imp_columns <- c(BIS_columns, "SSRT")
Patients_vs_HC_imp <- Patients_vs_HC[,c("Subject","Gruppe",imp_columns)]

## 3. Add confounds (Alter, Geschlecht, Abschluss)
Patients_vs_HC_imp_conf <- merge(Patients_vs_HC[,c("Subject","Alter","Geschlecht","Abschluss")], Patients_vs_HC_imp, by = "Subject")
```
Data for `r nrow(Patients_vs_HC_imp[Patients_vs_HC_imp$Gruppe == 1,])` patients and `r nrow(Patients_vs_HC_imp[Patients_vs_HC_imp$Gruppe == 0,])` healthy controls were available.
Find assumption test for normality in script "Testing Normality"

###Testing for variance homogeneity
```{r}
# Ensure "Gruppe" is treated as a factor
Patients_vs_HC_imp$Gruppe <- as.factor(Patients_vs_HC_imp$Gruppe)

# Function to perform Levene's test on multiple columns
levene_test_mult_cols <- function(df_basis, cols) {
  df <- data.frame(p_value = numeric(length(cols)))
  rownames(df) <- cols
  
  for (i in seq_along(cols)) {
    col <- cols[i]
    # Perform Levene's test, treating 'Gruppe' as a factor
    levene_result <- leveneTest(df_basis[[col]] ~ df_basis[["Gruppe"]])
    df[col, "p_value"] <- round(levene_result[1, "Pr(>F)"], 4)
  }
  
  return(df)
}

# Perform Levene's test for homogeneity of variance
levene_test_table <- levene_test_mult_cols(df_basis = Patients_vs_HC_imp, cols = imp_columns)
pander(levene_test_table, style = "rmarkdown", fontsize = "tiny")
```

### Calculate Welch t-test
```{r}
t_test_mult_cols <- function(df_basis, cols) {
  # Caclualte a t-test for multiple comparisons and store the results in a dataframe
  df <- data.frame(t_statistic = numeric(length(cols)), p_value = numeric(length(cols)),
                   group_mean_Patients = numeric(length(cols)), sd_Patients = numeric(length(cols)),
                   group_mean_HC = numeric(length(cols)), sd_HC = numeric(length(cols)),
                   cohen_d = numeric(length(cols)))
  rownames(df) <- cols
  p_values_raw <- numeric(length(cols))  # Vector to store raw p-values
  
  for (i in seq_along(cols)) {
    col <- cols[i] #seq_along for sequencing column indices
    
    # Using the non-formula interface
    group0 <- na.omit(df_basis[df_basis[["Gruppe"]] == 0, col])
    group1 <- na.omit(df_basis[df_basis[["Gruppe"]] == 1, col])
    results <- t.test(group0, group1, paired = FALSE, var.equal = FALSE)
    #Alternativ using formula method:
      #results <- t.test(df_basis[[col]] ~ df_basis[["Gruppe"]], paired = FALSE, var.equal = FALSE)
    
    # Store raw p-values for BH correction
    p_values_raw[i] <- results$p.value
    
    # Calculate Cohen's d using the cohen.d function (effect size)
    cohen_d_result <- cohen.d(group0, group1, hedges.correction = FALSE)
    cohen_d <- cohen_d_result$estimate
    
    # Calculate power using pwr.t.test
    n0 <- length(group0)
    n1 <- length(group1)
    power_result <- pwr.t.test(d = cohen_d, n = min(n0, n1), sig.level = 0.05, type = "two.sample", alternative = "greater")
    power <- power_result$power
    
    # Store the results
    df[col, "t_statistic"] <- round(results$statistic, 2)
    df[col, "p_value"] <- round(results$p.value, 2)
    df[col, "group_mean_Patients"] <- round(mean(group1), 2)
    df[col, "sd_Patients"] <- round(sd(group0), 2)
    df[col, "group_mean_HC"] <- round(mean(group0), 2)
    df[col, "sd_HC"] <- round(sd(group1), 2)
    df[col, "cohen_d"] <- round(cohen_d, 2)
    df[col, "power"] <- round(power, 2)
  }
  
  # Adjust p-values using Benjamini-Hochberg method for multiple testing of related tasks
  p_values_adjusted <- p.adjust(p_values_raw, method = "BH")
  df$p_value_adjusted <- round(p_values_adjusted, 2)
  
  return(df)
}

t_test_table <- t_test_mult_cols(df_basis = Patients_vs_HC_imp, cols = imp_columns)
pander(t_test_table, style = "rmarkdown", fontsize = "tiny")
```
Patients performed worse than healthy controls, especially in the Stroop and the NumberLetter Task. However, please note that we have not tested for baseline differences (e.g., education, age...). This is done in the following step.

### Control Welch-test for covariates age, sex, and education
#### Descriptive statistics
```{r}
#Descriptive statistics
table1 <- 
  Patients_vs_HC_imp_conf %>%
  tbl_summary(include = c(Alter, Geschlecht, Abschluss), by = Gruppe)
pander(as.data.frame(table1))
```
#### ANCOVA
```{r}
#ANCOVA
ANCOVA_mult_cols <- function(df_basis, cols, covariates = NULL){
  # Create a overview of ANCOVA results for several columns in a dataframe
  df <- data.frame(Gruppe_p_value = numeric(length(cols)))
  rownames(df) <- cols
  for (col in cols){
    # Run ANCOVA
    formula <- paste(col, " ~ Gruppe", ifelse(length(covariates) > 0, paste(" + ", paste(covariates, collapse = " + ")), ""))
    anova_result <- aov(as.formula(formula), data = df_basis)
    coefficients <- coef(anova_result)
    summary_anova <- summary(anova_result)
    #print(summary_anova)
    
    # Prepare nice table to summarize the results
    df[col, "Gruppe_p_value"] <- round(summary_anova[[1]]$"Pr(>F)"[1],2)
    df[col, "Gruppe_corrected_difference"] <- round(coefficients[2],2)
    df[col, "Alter_p_value"] <- round(summary_anova[[1]]$"Pr(>F)"[2],2)
    df[col, "Geschlecht_p_value"] <- round(summary_anova[[1]]$"Pr(>F)"[3],2)
    df[col, "Abschluss_p_value"] <- round(summary_anova[[1]]$"Pr(>F)"[4],2)
  }
  return(df)
}

table_ANCOVA <- ANCOVA_mult_cols(df_basis = Patients_vs_HC_imp_conf, cols = imp_columns, covariates = c("Alter","Geschlecht","Abschluss"))
pander(table_ANCOVA, style = "rmarkdown", fontsize = "tiny")
```
3 observations with missings in the covariates are automatically deleted.
In the Numberletter task, the difference between patients and healthy controls remained when controlling for covariates. However, also education had a main effect on performance.
In the Stroop task, the difference between patients and healthy controls also remained when contrlloing for covariates. Here, also the gender and education had a main effect on performance.
There was no significant effect between patients and controls in the TwoBackTask - also not when controlling for covariates. The task types Total and Foil might mainly depend on the age.

## <u>Comparison pre vs.post</u>

### Prepare dataset
```{r}
# 1. Identify common subjects in both datasets
data_Pat_pre$Subject_postID <- data_Pat_pre$Subject + 600 # IDs at post = ID at pre + 600

common_subjects_postID <- intersect(data_Pat_pre$Subject_postID, data_Pat_post$Subject) #Using intersect function to exclude only pre/post cases

# 2. Filter the datasets to only include common subjects
data_Pat_pre_filtered <- data_Pat_pre[data_Pat_pre$Subject_postID %in% common_subjects_postID , ]
data_Pat_post_filtered <- data_Pat_post[data_Pat_post$Subject %in% common_subjects_postID , ]

# 3. Important columns
data_Pat_pre_filtered_imp <- data_Pat_pre_filtered[, c("Subject", imp_columns)]
data_Pat_post_filtered_imp <- data_Pat_post_filtered[, c("Subject", imp_columns)]
```
Pre and post measures were available for `r nrow(data_Pat_pre_filtered_imp)` patients.

### Testing Assumption of dependent sample t-test: Positive correlation of measurement series
```{r}
## Calculate Raw Pearson Correlation and Test Significance
calculate_correlations_and_significance <- function(pre_data, post_data, cols) {
  df <- data.frame(Correlation = numeric(length(cols)), t_statistic = numeric(length(cols)), 
                   p_value = numeric(length(cols)))
  rownames(df) <- cols
  p_values_raw <- numeric(length(cols))
  for (i in seq_along(cols)) {
    col <- cols[i]
    
    # Use complete.cases to keep only rows where both pre AND post are not NA
    complete_cases <- complete.cases(pre_data[[col]], post_data[[col]])
    pre_col_data <- pre_data[[col]][complete_cases]
    post_col_data <- post_data[[col]][complete_cases]
    
    # Calculate Pearson correlation
    correlation <- cor(pre_col_data, post_col_data)
    
    # Calculate the t-statistic for the correlation
    n <- length(pre_col_data)
    t_statistic <- correlation * sqrt((n - 2) / (1 - correlation^2))
    
    # Calculate the p-value for a one-tailed test (H_0: r â‰¤ 0)
    p_value <- pt(t_statistic, df = n - 2, lower.tail = FALSE)
    p_values_raw[i] <- p_value
    
    # Store the results
    df[col, "Correlation"] <- round(correlation, 3)
    df[col, "t_statistic"] <- round(t_statistic, 3)
    df[col, "p_value"] <- round(p_value, 3)
  }
  
  # Adjust p-values using Benjamini-Hochberg method
  p_values_adjusted <- p.adjust(p_values_raw, method = "BH")
  df$p_value_adjusted <- round(p_values_adjusted, 2)
  
  return(df)
}


# Apply the correlation and significance test
correlation_significance_table <- calculate_correlations_and_significance(pre_data = data_Pat_pre_filtered_imp, post_data = data_Pat_post_filtered_imp, cols = imp_columns)
pander(correlation_significance_table, style = "rmarkdown", fontsize = "tiny")
```
For all tasks, the correlations of the measurement series are significantly positive. Thus, the assumption is met for all tasks. 

### Calculate Welch t-test
```{r}
paired_t_test_mult_cols <- function(pre_data, post_data, cols) {
  df <- data.frame(t_statistic = numeric(length(cols)), p_value = numeric(length(cols)),
                   group_mean_pre = numeric(length(cols)), group_mean_post = numeric(length(cols)),
                   cohen_d = numeric(length(cols)))
  rownames(df) <- cols
  p_values_raw <- numeric(length(cols)) # Vector to store raw p-values
  
  for (i in seq_along(cols)) {
    col <- cols[i]
    
    # Use complete.cases to keep only rows where both pre AND post are not NA
    complete_cases <- complete.cases(pre_data[[col]], post_data[[col]])
    pre_col_data <- pre_data[[col]][complete_cases]
    post_col_data <- post_data[[col]][complete_cases]
    
    # Check if there are at least 2 complete pairs of observations
    if (length(pre_col_data) < 2 || length(post_col_data) < 2) {
      warning(paste("Not enough data for column:", col))
      df[col, ] <- NA  # Assign NA to the row in case of insufficient data
      next  # Skip to the next column
    }
    
    # Perform paired Welch t-test (non-formula interface)
    results <- t.test(pre_col_data, post_col_data, paired = TRUE, var.equal = FALSE)
    #Alternativ using formula method:
    #results <- t.test(df_basis[[col]] ~ df_basis[["time"]], paired = TRUE, var.equal = FALSE)
    
    # Store raw p-values for BH correction
    p_values_raw[i] <- results$p.value
    
    # Calculate Cohen's d for paired samples (effect size)
    cohen_d_result <- cohen.d(post_col_data, pre_col_data, paired = TRUE)
    cohen_d <- cohen_d_result$estimate
    
    # Calculate power using pwr.t.test
    n <- length(pre_col_data)
    power_result <- pwr.t.test(d = cohen_d, n = n, sig.level = 0.05, type = "paired", alternative = "greater")
    power <- power_result$power
    
    # Store the results
    df[col, "t_statistic"] <- round(results$statistic, 2)
    df[col, "p_value"] <- round(results$p.value, 2)
    df[col, "group_mean_pre"] <- round(mean(pre_col_data), 2)
    df[col, "sd_pre"] <- round(sd(pre_col_data), 2)
    df[col, "group_mean_post"] <- round(mean(post_col_data), 2)
    df[col, "sd_post"] <- round(sd(post_col_data), 2)
    df[col, "cohen_d"] <- round(cohen_d, 2)
    df[col, "power"] <- round(power, 2)
  }
  
  # Adjust p-values using Benjamini-Hochberg method for multiple testing of related tasks
  p_values_adjusted <- p.adjust(p_values_raw, method = "BH")
  df$p_value_adjusted <- round(p_values_adjusted, 2)
  
  return(df)
}

# Apply the paired Welch t-test to your data
t_test_table_pre_post <- paired_t_test_mult_cols(pre_data = data_Pat_pre_filtered_imp, post_data = data_Pat_post_filtered_imp, cols = imp_columns)
pander(t_test_table_pre_post)
```
Patients performed better in all measurements of executive functioning after the exposure session. This might be due to a practice effect.

## <u>Plotting</u>

### Comparison HC vs. patients (pre)
#### Prepare data; TODO: include SSRT or do separate plot?
```{r}
# Reshape data from wide to long format
Patients_vs_HC_long <- Patients_vs_HC_imp %>%
  pivot_longer(cols = all_of(BIS_columns), # imp_columns to include SSRT
               names_to = "Condition",
               values_to = "BIS_Score")

# Add new task variable to keep the headers intact for faceting
Patients_vs_HC_long <- Patients_vs_HC_long %>%
  mutate(Task = case_when(
    grepl("NumberLetter", Condition) ~ "NumberLetter",
    grepl("Stroop", Condition) ~ "Stroop",
    grepl("TwoBack", Condition) ~ "TwoBack"
  ))

# Remove prefixes from Condition labels, keeping Task intact
Patients_vs_HC_long <- Patients_vs_HC_long %>%
  mutate(Condition = gsub("NumberLetter_BIS_|Stroop_BIS_|TwoBack_BIS_", "", Condition))

# Convert "Gruppe" to factor # TODO: change order of the conditions for consistency
Patients_vs_HC_long$Gruppe <- factor(Patients_vs_HC_long$Gruppe, levels = c(0, 1), labels = c("Healthy Controls", "Patients"))

```

#### Create plots
```{r}
# Create boxplots
boxplots_Patients_HC <- ggplot(Patients_vs_HC_long, aes(x = Condition, y = BIS_Score, color = Gruppe)) +
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(position = position_jitterdodge(),
              size = 0.7,
              shape = 21,
              fill = NA) +
  facet_wrap(~ Task, scales = "free_x") +
  #facet_grid(rows = vars(Task), scales = "free_x") +
  labs(title = "Boxplots of Task Condition by Group",
       x = "Condition",
       y = "BIS-Score",
       color = "Group")

# Create violin plots with mean
violin_plots_patients_hc <- ggplot(Patients_vs_HC_long, aes(x = Condition, y = BIS_Score, fill = Gruppe)) +
  geom_violin(position = position_dodge(width = 0.8)) +
  stat_summary(fun.data = mean_sdl, geom = "pointrange", # mult = 2 (SD) by default
               position = position_dodge(width = 0.8)) + # width = 0.65, fatten = 2
  facet_wrap(~ Task, scales = "free_x") +
  labs(title = "Violin Plots of Task Condition by Group",
       x = "Task Condition",
       y = "BIS-Score",
       fill = "Group")
```

##### Boxplot: Patients vs. Healthy Controls
```{r}
print(boxplots_Patients_HC)
```

##### Violin Plot: Patients vs. Healthy Controls
```{r}
print(violin_plots_patients_hc)
```

#### Comparison patients pre vs. post
# TODO: include SSRT or do separate plot?
```{r}
# Add a timepoint variable to the filtered datasets and combine
data_Pat_pre_filtered_imp$time <- "pre"
data_Pat_post_filtered_imp$time <- "post"
Patients_pre_vs_post_long <- rbind(data_Pat_pre_filtered_imp, data_Pat_post_filtered_imp)

# Reshape data from wide to long format
Patients_pre_vs_post_long <- Patients_pre_vs_post_long %>%
  pivot_longer(cols = all_of(BIS_columns), # imp_columns to include SSRT
               names_to = "Condition",
               values_to = "BIS_Score")

# Add new task variable
Patients_pre_vs_post_long <- Patients_pre_vs_post_long %>%
  mutate(Task = case_when(
    grepl("NumberLetter", Condition) ~ "NumberLetter",
    grepl("Stroop", Condition) ~ "Stroop",
    grepl("TwoBack", Condition) ~ "TwoBack"
  ))

# Remove prefixes from Condition labels, keeping Task intact
Patients_pre_vs_post_long <- Patients_pre_vs_post_long %>%
  mutate(Condition = gsub("NumberLetter_BIS_|Stroop_BIS_|TwoBack_BIS_", "", Condition))

# Convert "time" to factor, specify order
Patients_pre_vs_post_long$time <- factor(Patients_pre_vs_post_long$time, levels = c("pre", "post"))

# Create boxplot with paired observations
boxplot_Patients_pre_post <- ggplot(Patients_pre_vs_post_long, aes(x = Condition, y = BIS_Score, color = time)) + # group = Subject
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(position = position_jitterdodge(),
              size = 0.7,
              shape = 21,
              fill = NA) +
  #geom_line(aes(group = Subject)) + # optional, if you want to visualize pairing
  facet_wrap(~ Task, scales = "free_x") +
  labs(title = "Boxplots of Task Condition by Time of Measurement",
       x = "Condition",
       y = "BIS-Score",
       color = "Time")
```

##### Boxplot for Patients pre vs. post intervention
```{r}
print(boxplot_Patients_pre_post)
```

## <u>Comparison Response vs. Non-Response</u>

###Testing for variance homogeneity
```{r}
# Ensure "Response" is treated as a factor
data_Pat_pre$Response <- as.factor(data_Pat_pre$Response)

# Perform Levene's test for homogeneity of variance on task performance measures
levene_test_mult_cols <- function(df_basis, cols) {
  df <- data.frame(p_value = numeric(length(cols)))
  rownames(df) <- cols
  
  for (i in seq_along(cols)) {
    col <- cols[i]
    # Perform Levene's test, treating 'Response' as a factor
    levene_result <- leveneTest(df_basis[[col]] ~ df_basis[["Response"]])
    df[col, "p_value"] <- round(levene_result[1, "Pr(>F)"], 4)
  }
  
  return(df)
}

# Define the columns for which to perform the test (e.g., task performance measures)
imp_columns <- c("NumberLetter_BIS_Repeat", "NumberLetter_BIS_Switch", 
                 "NumberLetter_BIS_Diff_Score", "Stroop_BIS_Congruent", 
                 "Stroop_BIS_Incongruent", "Stroop_BIS_Diff_Score", 
                 "TwoBack_BIS_Foil", "TwoBack_BIS_Target", "TwoBack_BIS_Total", 
                 "SSRT")

# Perform Levene's test
levene_test_table <- levene_test_mult_cols(df_basis = data_Pat_pre, cols = imp_columns)
pander(levene_test_table, style = "rmarkdown", fontsize = "tiny")
```
Levene test shows complete variance homoegeneity for all task conditions between responders and non-responders.


### **Calculate t-test**
```{r}
# Perform t-tests for each task and store results
t_test_mult_cols <- function(df_basis, cols) {
  df <- data.frame(t_statistic = numeric(length(cols)), p_value = numeric(length(cols)),
                   group_mean_Response = numeric(length(cols)), sd_Response = numeric(length(cols)),
                   group_mean_NonResponse = numeric(length(cols)), sd_NonResponse = numeric(length(cols)),
                   cohen_d = numeric(length(cols)))
  rownames(df) <- cols
  
  for (i in seq_along(cols)) {
    col <- cols[i]
    
    # Separate groups by Response = 1 and Response = 0
    group1 <- na.omit(df_basis[df_basis[["Response"]] == 1, col])
    group0 <- na.omit(df_basis[df_basis[["Response"]] == 0, col])
    #Alternativ using formula method:
      #results <- t.test(df_basis[[col]] ~ df_basis[["Response"]], paired = FALSE, var.equal = TRUE)
    
    # Perform standard t-test (since variance homogeneity holds)
    results <- t.test(group1, group0, paired = FALSE, var.equal = TRUE)
    
    # Calculate Cohen's d
    cohen_d_result <- cohen.d(group1, group0, hedges.correction = FALSE)
    cohen_d <- cohen_d_result$estimate
    
    # Calculate power
    n0 <- length(group0)
    n1 <- length(group1)
    power_result <- pwr.t.test(d = cohen_d, n = min(n0, n1), sig.level = 0.05, type = "two.sample", alternative = "greater")
    power <- power_result$power
    
    # Store the results
    df[col, "t_statistic"] <- round(results$statistic, 2)
    df[col, "p_value"] <- round(results$p.value, 4)
    df[col, "group_mean_Response"] <- round(mean(group1), 2)
    df[col, "sd_Response"] <- round(sd(group1), 2)
    df[col, "group_mean_NonResponse"] <- round(mean(group0), 2)
    df[col, "sd_NonResponse"] <- round(sd(group0), 2)
    df[col, "cohen_d"] <- round(cohen_d, 2)
    df[col, "power"] <- round(power, 2)
  }
  
  return(df)
}

t_test_table <- t_test_mult_cols(df_basis = data_Pat_pre, cols = imp_columns)
pander(t_test_table, style = "rmarkdown", split.table = Inf, fontsize = "tiny")
```
No significant was found for any of the tasks.
Potential reason: low power. Even the largest power for TwoBack_BIS_Foil of 0.22 with a cohen's d of 0.2 would require a sample size of 144 to find a significant effect, as a priori power power analysis shows. However, the patient group is only about half as large as that (n=89), not to speak of the tasks with even smaller power (e.g., NumberLetter_BIS_Repeat with 0.01)
